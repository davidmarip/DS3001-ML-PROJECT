{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3970f2c",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning Modeling\n",
    "## Ensemble Methods for Food Health Classification\n",
    "\n",
    "This notebook implements advanced machine learning approaches including:\n",
    "- Multiple algorithm comparison (Random Forest, XGBoost, Neural Networks)\n",
    "- Ensemble methods and model stacking\n",
    "- Advanced cross-validation strategies\n",
    "- Feature importance analysis\n",
    "- Model interpretability (SHAP values)\n",
    "\n",
    "Goal: Classify foods as healthy/unhealthy using comprehensive feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c256f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core ML libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, StratifiedKFold, \n",
    "    GridSearchCV, RandomizedSearchCV\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "# Advanced ML libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Model interpretation\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SHAP_AVAILABLE = False\n",
    "    print(\"SHAP not available - install with: pip install shap\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup directories\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "results_dir = Path('../RESULTS')\n",
    "models_dir = results_dir / 'models'\n",
    "figures_dir = results_dir / 'figures'\n",
    "reports_dir = results_dir / 'reports'\n",
    "\n",
    "for directory in [results_dir, models_dir, figures_dir, reports_dir]:\n",
    "    directory.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Advanced ML libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7c4fa0",
   "metadata": {},
   "source": [
    "## Target Variable Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fa82f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HealthClassifier:\n",
    "    \"\"\"Advanced health classification system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Health scoring weights\n",
    "        self.health_weights = {\n",
    "            'ingredient_health_score': 0.3,\n",
    "            'preservatives_score': -0.2,\n",
    "            'artificial_colors_score': -0.25,\n",
    "            'artificial_sweeteners_score': -0.15,\n",
    "            'natural_sweeteners_score': 0.1,\n",
    "            'whole_grains_score': 0.2,\n",
    "            'healthy_fats_score': 0.15,\n",
    "            'processing_claims_count': 0.1,\n",
    "            'complexity_score': -0.1,\n",
    "            'category_health_score': 0.2\n",
    "        }\n",
    "    \n",
    "    def create_health_labels(self, features_df):\n",
    "        \"\"\"Create binary health labels using weighted scoring\"\"\"\n",
    "        health_score = np.zeros(len(features_df))\n",
    "        \n",
    "        for feature, weight in self.health_weights.items():\n",
    "            if feature in features_df.columns:\n",
    "                # Normalize feature to 0-1 scale\n",
    "                feature_values = features_df[feature].fillna(0)\n",
    "                if feature_values.max() > 0:\n",
    "                    normalized = feature_values / feature_values.max()\n",
    "                    health_score += weight * normalized\n",
    "        \n",
    "        # Convert to binary labels (top 30% as healthy)\n",
    "        threshold = np.percentile(health_score, 70)\n",
    "        health_labels = (health_score >= threshold).astype(int)\n",
    "        \n",
    "        return health_labels, health_score\n",
    "    \n",
    "    def get_feature_importance_weights(self):\n",
    "        \"\"\"Return feature importance weights for interpretation\"\"\"\n",
    "        return self.health_weights\n",
    "\n",
    "health_classifier = HealthClassifier()\n",
    "print(\"Health classification system ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89380035",
   "metadata": {},
   "source": [
    "## Model Pipeline & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61ef28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedModelPipeline:\n",
    "    \"\"\"Advanced machine learning pipeline with multiple algorithms\"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.models = {}\n",
    "        self.best_params = {}\n",
    "        self.results = {}\n",
    "        \n",
    "        # Define model configurations\n",
    "        self.model_configs = {\n",
    "            'random_forest': {\n",
    "                'model': RandomForestClassifier(random_state=random_state),\n",
    "                'params': {\n",
    "                    'n_estimators': [100, 200, 300],\n",
    "                    'max_depth': [10, 20, None],\n",
    "                    'min_samples_split': [2, 5, 10],\n",
    "                    'min_samples_leaf': [1, 2, 4]\n",
    "                }\n",
    "            },\n",
    "            'xgboost': {\n",
    "                'model': xgb.XGBClassifier(random_state=random_state, eval_metric='logloss'),\n",
    "                'params': {\n",
    "                    'n_estimators': [100, 200, 300],\n",
    "                    'max_depth': [3, 6, 9],\n",
    "                    'learning_rate': [0.01, 0.1, 0.2],\n",
    "                    'subsample': [0.8, 0.9, 1.0]\n",
    "                }\n",
    "            },\n",
    "            'neural_network': {\n",
    "                'model': MLPClassifier(random_state=random_state, max_iter=1000),\n",
    "                'params': {\n",
    "                    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "                    'activation': ['relu', 'tanh'],\n",
    "                    'alpha': [0.0001, 0.001, 0.01],\n",
    "                    'learning_rate': ['constant', 'adaptive']\n",
    "                }\n",
    "            },\n",
    "            'logistic_regression': {\n",
    "                'model': LogisticRegression(random_state=random_state, max_iter=1000),\n",
    "                'params': {\n",
    "                    'C': [0.1, 1, 10, 100],\n",
    "                    'penalty': ['l1', 'l2'],\n",
    "                    'solver': ['liblinear', 'saga']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def train_individual_models(self, X_train, y_train, X_test, y_test, cv_folds=5):\n",
    "        \"\"\"Train and tune individual models\"\"\"\n",
    "        print(\"Training individual models with hyperparameter tuning...\")\n",
    "        \n",
    "        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.random_state)\n",
    "        \n",
    "        for name, config in self.model_configs.items():\n",
    "            print(f\"   ðŸ”§ Tuning {name}...\")\n",
    "            \n",
    "            # Randomized search for efficiency\n",
    "            search = RandomizedSearchCV(\n",
    "                config['model'],\n",
    "                config['params'],\n",
    "                n_iter=20,  # Limit iterations for speed\n",
    "                cv=cv,\n",
    "                scoring='f1',\n",
    "                n_jobs=-1,\n",
    "                random_state=self.random_state\n",
    "            )\n",
    "            \n",
    "            search.fit(X_train, y_train)\n",
    "            \n",
    "            # Store best model and parameters\n",
    "            self.models[name] = search.best_estimator_\n",
    "            self.best_params[name] = search.best_params_\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            y_pred = search.best_estimator_.predict(X_test)\n",
    "            y_pred_proba = search.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            self.results[name] = {\n",
    "                'accuracy': accuracy_score(y_test, y_pred),\n",
    "                'precision': precision_score(y_test, y_pred),\n",
    "                'recall': recall_score(y_test, y_pred),\n",
    "                'f1': f1_score(y_test, y_pred),\n",
    "                'auc': roc_auc_score(y_test, y_pred_proba),\n",
    "                'cv_score': search.best_score_\n",
    "            }\n",
    "            \n",
    "            print(f\"     {name}: F1={self.results[name]['f1']:.3f}, AUC={self.results[name]['auc']:.3f}\")\n",
    "        \n",
    "        return self.models, self.results\n",
    "    \n",
    "    def create_ensemble_models(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Create ensemble models using voting and stacking\"\"\"\n",
    "        print(\"ðŸ”— Creating ensemble models...\")\n",
    "        \n",
    "        # Voting Classifier\n",
    "        voting_clf = VotingClassifier(\n",
    "            estimators=[\n",
    "                ('rf', self.models['random_forest']),\n",
    "                ('xgb', self.models['xgboost']),\n",
    "                ('lr', self.models['logistic_regression'])\n",
    "            ],\n",
    "            voting='soft'\n",
    "        )\n",
    "        \n",
    "        voting_clf.fit(X_train, y_train)\n",
    "        self.models['voting_ensemble'] = voting_clf\n",
    "        \n",
    "        # Stacking Classifier\n",
    "        stacking_clf = StackingClassifier(\n",
    "            estimators=[\n",
    "                ('rf', self.models['random_forest']),\n",
    "                ('xgb', self.models['xgboost']),\n",
    "                ('nn', self.models['neural_network'])\n",
    "            ],\n",
    "            final_estimator=LogisticRegression(random_state=self.random_state),\n",
    "            cv=5\n",
    "        )\n",
    "        \n",
    "        stacking_clf.fit(X_train, y_train)\n",
    "        self.models['stacking_ensemble'] = stacking_clf\n",
    "        \n",
    "        # Evaluate ensemble models\n",
    "        for ensemble_name in ['voting_ensemble', 'stacking_ensemble']:\n",
    "            y_pred = self.models[ensemble_name].predict(X_test)\n",
    "            y_pred_proba = self.models[ensemble_name].predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            self.results[ensemble_name] = {\n",
    "                'accuracy': accuracy_score(y_test, y_pred),\n",
    "                'precision': precision_score(y_test, y_pred),\n",
    "                'recall': recall_score(y_test, y_pred),\n",
    "                'f1': f1_score(y_test, y_pred),\n",
    "                'auc': roc_auc_score(y_test, y_pred_proba)\n",
    "            }\n",
    "            \n",
    "            print(f\"   {ensemble_name}: F1={self.results[ensemble_name]['f1']:.3f}, AUC={self.results[ensemble_name]['auc']:.3f}\")\n",
    "        \n",
    "        return self.models\n",
    "    \n",
    "    def get_best_model(self):\n",
    "        \"\"\"Return the best performing model based on F1 score\"\"\"\n",
    "        best_model_name = max(self.results.keys(), key=lambda x: self.results[x]['f1'])\n",
    "        return best_model_name, self.models[best_model_name]\n",
    "    \n",
    "    def save_models(self):\n",
    "        \"\"\"Save all trained models to disk\"\"\"\n",
    "        print(\"Saving trained models...\")\n",
    "        \n",
    "        for model_name, model in self.models.items():\n",
    "            model_path = models_dir / f'{model_name}_model.pkl'\n",
    "            joblib.dump(model, model_path)\n",
    "            print(f\"  Saved {model_name} to {model_path}\")\n",
    "        \n",
    "        # Save results summary\n",
    "        results_path = models_dir / 'model_results.json'\n",
    "        import json\n",
    "        with open(results_path, 'w') as f:\n",
    "            json.dump(self.results, f, indent=2)\n",
    "        \n",
    "        print(f\"  Saved results summary to {results_path}\")\n",
    "        return len(self.models)\n",
    "\n",
    "print(\"Advanced model pipeline ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29033bd",
   "metadata": {},
   "source": [
    "## Model Evaluation & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c7983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_evaluation(results, models, X_test, y_test, feature_names):\n",
    "    \"\"\"Create comprehensive model evaluation dashboard\"\"\"\n",
    "    \n",
    "    # 1. Performance Comparison Chart\n",
    "    metrics_df = pd.DataFrame(results).T\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=['F1 Score', 'Accuracy', 'Precision', 'Recall'],\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"bar\"}]]\n",
    "    )\n",
    "    \n",
    "    metrics = ['f1', 'accuracy', 'precision', 'recall']\n",
    "    positions = [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
    "    \n",
    "    for metric, (row, col) in zip(metrics, positions):\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=metrics_df.index,\n",
    "                y=metrics_df[metric],\n",
    "                name=metric.title(),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Model Performance Comparison\",\n",
    "        height=600\n",
    "    )\n",
    "    fig.write_html(figures_dir / 'model_performance_comparison.html')\n",
    "    fig.show()\n",
    "    \n",
    "    # 2. Feature Importance (for tree-based models)\n",
    "    if 'random_forest' in models:\n",
    "        rf_importance = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': models['random_forest'].feature_importances_\n",
    "        }).sort_values('importance', ascending=False).head(15)\n",
    "        \n",
    "        fig_importance = px.bar(\n",
    "            rf_importance,\n",
    "            x='importance',\n",
    "            y='feature',\n",
    "            orientation='h',\n",
    "            title=\"Top 15 Feature Importances (Random Forest)\"\n",
    "        )\n",
    "        fig_importance.write_html(figures_dir / 'feature_importance_rf.html')\n",
    "        fig_importance.show()\n",
    "    \n",
    "    # 3. Results Summary Table\n",
    "    print(\"\\n MODEL PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    summary_df = metrics_df[['f1', 'accuracy', 'precision', 'recall', 'auc']].round(3)\n",
    "    print(summary_df.to_string())\n",
    "    \n",
    "    # Best model recommendation\n",
    "    best_f1_model = summary_df['f1'].idxmax()\n",
    "    best_auc_model = summary_df['auc'].idxmax()\n",
    "    \n",
    "    print(f\"\\nBEST MODELS:\")\n",
    "    print(f\"   Best F1 Score: {best_f1_model} ({summary_df.loc[best_f1_model, 'f1']:.3f})\")\n",
    "    print(f\"   Best AUC Score: {best_auc_model} ({summary_df.loc[best_auc_model, 'auc']:.3f})\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "print(\"Evaluation framework ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c2bef9",
   "metadata": {},
   "source": [
    "## Model Interpretability with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4729411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_interpretability(model, X_test, feature_names):\n",
    "    \"\"\"Analyze model interpretability using SHAP values\"\"\"\n",
    "    \n",
    "    if not SHAP_AVAILABLE:\n",
    "        print(\"SHAP not available for model interpretability analysis\")\n",
    "        return\n",
    "    \n",
    "    print(\"Analyzing model interpretability with SHAP...\")\n",
    "    \n",
    "    try:\n",
    "        # Create SHAP explainer\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            explainer = shap.Explainer(model, X_test.sample(100))\n",
    "            shap_values = explainer(X_test.sample(200))\n",
    "            \n",
    "            # Summary plot\n",
    "            shap.summary_plot(shap_values, X_test.sample(200), feature_names=feature_names, show=False)\n",
    "            plt.title(\"SHAP Feature Importance Summary\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Waterfall plot for a single prediction\n",
    "            shap.waterfall_plot(shap_values[0], show=False)\n",
    "            plt.title(\"SHAP Waterfall Plot (Single Prediction)\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"SHAP analysis complete\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"SHAP analysis failed: {str(e)}\")\n",
    "\n",
    "print(\"Model interpretability framework ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84064058",
   "metadata": {},
   "source": [
    "## Main Modeling Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417215b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell would execute the full modeling pipeline with real data\n",
    "def run_complete_modeling_pipeline(features_df, test_size=0.2):\n",
    "    \"\"\"Execute the complete advanced modeling pipeline\"\"\"\n",
    "    \n",
    "    print(\"STARTING ADVANCED MODELING PIPELINE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Create target variable\n",
    "    print(\"\\n Creating health classification labels...\")\n",
    "    y, health_scores = health_classifier.create_health_labels(features_df)\n",
    "    \n",
    "    # Select features for modeling\n",
    "    feature_columns = [\n",
    "        'brand_product_count', 'brand_category_diversity', 'brand_premium_score',\n",
    "        'preservatives_score', 'artificial_colors_score', 'artificial_sweeteners_score',\n",
    "        'natural_sweeteners_score', 'whole_grains_score', 'healthy_fats_score',\n",
    "        'processing_claims_count', 'ingredient_count', 'complexity_score',\n",
    "        'ingredient_health_score', 'category_frequency', 'category_health_score'\n",
    "    ]\n",
    "    \n",
    "    # Filter available features\n",
    "    available_features = [f for f in feature_columns if f in features_df.columns]\n",
    "    X = features_df[available_features].fillna(0)\n",
    "    \n",
    "    print(f\"   Using {len(available_features)} features for modeling\")\n",
    "    print(f\"   Target distribution: {np.bincount(y)} (Healthy: {y.sum()}, Unhealthy: {len(y)-y.sum()})\")\n",
    "    \n",
    "    # 2. Train-test split\n",
    "    print(\"\\nSplitting data...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # 3. Initialize and run pipeline\n",
    "    print(\"\\n Training models...\")\n",
    "    pipeline = AdvancedModelPipeline()\n",
    "    \n",
    "    # Train individual models\n",
    "    models, results = pipeline.train_individual_models(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # Create ensemble models\n",
    "    ensemble_models = pipeline.create_ensemble_models(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # 4. Comprehensive evaluation\n",
    "    print(\"\\n Model evaluation...\")\n",
    "    summary_df = create_comprehensive_evaluation(\n",
    "        pipeline.results, pipeline.models, X_test, y_test, available_features\n",
    "    )\n",
    "    \n",
    "    # 5. Model interpretability\n",
    "    print(\"\\n Model interpretability analysis...\")\n",
    "    best_model_name, best_model = pipeline.get_best_model()\n",
    "    analyze_model_interpretability(best_model, X_test, available_features)\n",
    "    \n",
    "    print(f\"\\n MODELING PIPELINE COMPLETE!\")\n",
    "    print(f\"   Best model: {best_model_name}\")\n",
    "    print(f\"   Best F1 score: {pipeline.results[best_model_name]['f1']:.3f}\")\n",
    "    \n",
    "    return pipeline, summary_df, best_model_name\n",
    "\n",
    "# Placeholder for execution with real data\n",
    "print(\"Complete modeling pipeline ready for execution\")\n",
    "print(\"   Run: pipeline, summary, best_model = run_complete_modeling_pipeline(features_df)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc08293",
   "metadata": {},
   "source": [
    "## Modeling Summary & Insights\n",
    "\n",
    "### Advanced ML Pipeline Features:\n",
    "\n",
    "1. **Multi-Algorithm Comparison**:\n",
    "   - Random Forest with hyperparameter tuning\n",
    "   - XGBoost with gradient boosting optimization\n",
    "   - Neural Networks with adaptive learning\n",
    "   - Logistic Regression baseline\n",
    "\n",
    "2. **Ensemble Methods**:\n",
    "   - Voting Classifier (soft voting)\n",
    "   - Stacking Classifier with meta-learner\n",
    "   - Performance comparison across methods\n",
    "\n",
    "3. **Advanced Evaluation**:\n",
    "   - Stratified cross-validation\n",
    "   - Multiple metrics (F1, AUC, Precision, Recall)\n",
    "   - Feature importance analysis\n",
    "   - Model interpretability with SHAP\n",
    "\n",
    "### Expected Outcomes:\n",
    "- **Model Performance**: F1 scores likely 0.75-0.85 range\n",
    "- **Key Features**: Ingredient health scores, preservatives, processing claims\n",
    "- **Best Algorithm**: Likely ensemble methods or XGBoost\n",
    "- **Interpretability**: Clear feature contribution analysis\n",
    "\n",
    "### Business Value:\n",
    "- Automated food health classification\n",
    "- Ingredient quality assessment\n",
    "- Brand positioning insights\n",
    "- Consumer health guidance\n",
    "\n",
    "**Next Notebook**: `05_Evaluation.ipynb` - Detailed model validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd10f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the complete modeling pipeline (placeholder for actual execution)\n",
    "print(\"MACHINE LEARNING PIPELINE EXECUTION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Note: This cell would typically load the engineered features and run the pipeline\n",
    "# For demonstration, we show the expected workflow:\n",
    "\n",
    "print(\"Step 1: Loading engineered features...\")\n",
    "# features_df = pd.read_pickle('../RESULTS/features/engineered_features.pkl')\n",
    "\n",
    "print(\"Step 2: Creating health labels...\")\n",
    "# health_classifier = HealthClassifier()\n",
    "# y = health_classifier.create_health_labels(features_df)\n",
    "\n",
    "print(\"Step 3: Training models...\")\n",
    "# pipeline = AdvancedModelPipeline()\n",
    "# models, results = pipeline.train_individual_models(X_train, y_train, X_test, y_test)\n",
    "# ensemble_models = pipeline.train_ensemble_methods(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"Step 4: Saving models...\")\n",
    "# pipeline.save_models()\n",
    "\n",
    "print(\"Step 5: Generating evaluation visualizations...\")\n",
    "# evaluation_summary = create_comprehensive_evaluation(\n",
    "#     pipeline.results, pipeline.models, X_test, y_test, available_features\n",
    "# )\n",
    "\n",
    "print(\"Step 6: Selecting best model...\")\n",
    "# best_model_name, best_model = pipeline.get_best_model()\n",
    "\n",
    "print(\"\\nPipeline complete! All models and results saved to RESULTS/models/\")\n",
    "print(\"Visualizations saved to RESULTS/figures/\")\n",
    "print(\"Proceed to 05_Evaluation.ipynb for detailed analysis\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
