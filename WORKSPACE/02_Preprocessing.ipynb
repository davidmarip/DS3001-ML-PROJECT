{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0270cd06",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Cleaning\n",
    "## USDA FoodData Central - Branded Foods Dataset\n",
    "\n",
    "This notebook handles data cleaning, preprocessing, and preparation for analysis including:\n",
    "- Data loading and initial assessment\n",
    "- Missing value treatment\n",
    "- Data type conversions and standardization\n",
    "- Text preprocessing for ingredients\n",
    "- Data quality validation\n",
    "- Export of cleaned datasets\n",
    "\n",
    "Input: Raw CSV files from DATA directory\n",
    "Output: Cleaned datasets saved to RESULTS directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97bfac93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded and directories created\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create results directories\n",
    "results_dir = Path('../RESULTS')\n",
    "data_dir = results_dir / 'processed_data'\n",
    "figures_dir = results_dir / 'figures'\n",
    "models_dir = results_dir / 'models'\n",
    "\n",
    "for directory in [results_dir, data_dir, figures_dir, models_dir]:\n",
    "    directory.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Libraries loaded and directories created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5565333",
   "metadata": {},
   "source": [
    "## Data Loading and Initial Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2710b948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.53 GiB for an array with shape (8, 25652681) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading datasets...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m branded_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../DATA/branded_food.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m nutrient_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../DATA/food_nutrient.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m nutrient_ref \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../DATA/nutrient.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m food_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../DATA/food.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Tony Tran\\Documents\\DS3001-ML-PROJECT-1\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tony Tran\\Documents\\DS3001-ML-PROJECT-1\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tony Tran\\Documents\\DS3001-ML-PROJECT-1\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1968\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1965\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1966\u001b[0m         new_col_dict \u001b[38;5;241m=\u001b[39m col_dict\n\u001b[1;32m-> 1968\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_col_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1973\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1975\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_rows\n\u001b[0;32m   1976\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\Tony Tran\\Documents\\DS3001-ML-PROJECT-1\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:782\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    776\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    777\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    778\u001b[0m     )\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 782\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mc:\\Users\\Tony Tran\\Documents\\DS3001-ML-PROJECT-1\\.venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tony Tran\\Documents\\DS3001-ML-PROJECT-1\\.venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:152\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    149\u001b[0m axes \u001b[38;5;241m=\u001b[39m [columns, index]\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_block_manager_from_column_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefs\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
      "File \u001b[1;32mc:\\Users\\Tony Tran\\Documents\\DS3001-ML-PROJECT-1\\.venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2158\u001b[0m, in \u001b[0;36mcreate_block_manager_from_column_arrays\u001b[1;34m(arrays, axes, consolidate, refs)\u001b[0m\n\u001b[0;32m   2140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_block_manager_from_column_arrays\u001b[39m(\n\u001b[0;32m   2141\u001b[0m     arrays: \u001b[38;5;28mlist\u001b[39m[ArrayLike],\n\u001b[0;32m   2142\u001b[0m     axes: \u001b[38;5;28mlist\u001b[39m[Index],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2154\u001b[0m     \u001b[38;5;66;03m# These last three are sufficient to allow us to safely pass\u001b[39;00m\n\u001b[0;32m   2155\u001b[0m     \u001b[38;5;66;03m#  verify_integrity=False below.\u001b[39;00m\n\u001b[0;32m   2157\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2158\u001b[0m         blocks \u001b[38;5;241m=\u001b[39m \u001b[43m_form_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2159\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m BlockManager(blocks, axes, verify_integrity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   2160\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Tony Tran\\Documents\\DS3001-ML-PROJECT-1\\.venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2231\u001b[0m, in \u001b[0;36m_form_blocks\u001b[1;34m(arrays, consolidate, refs)\u001b[0m\n\u001b[0;32m   2228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(dtype\u001b[38;5;241m.\u001b[39mtype, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[0;32m   2229\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;28mobject\u001b[39m)\n\u001b[1;32m-> 2231\u001b[0m values, placement \u001b[38;5;241m=\u001b[39m \u001b[43m_stack_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtup_block\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dtlike:\n\u001b[0;32m   2233\u001b[0m     values \u001b[38;5;241m=\u001b[39m ensure_wrapped_if_datetimelike(values)\n",
      "File \u001b[1;32mc:\\Users\\Tony Tran\\Documents\\DS3001-ML-PROJECT-1\\.venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2271\u001b[0m, in \u001b[0;36m_stack_arrays\u001b[1;34m(tuples, dtype)\u001b[0m\n\u001b[0;32m   2268\u001b[0m first \u001b[38;5;241m=\u001b[39m arrays[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   2269\u001b[0m shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(arrays),) \u001b[38;5;241m+\u001b[39m first\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m-> 2271\u001b[0m stacked \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2272\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, arr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arrays):\n\u001b[0;32m   2273\u001b[0m     stacked[i] \u001b[38;5;241m=\u001b[39m arr\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.53 GiB for an array with shape (8, 25652681) and data type float64"
     ]
    }
   ],
   "source": [
    "# Load main datasets\n",
    "print(\"Loading datasets...\")\n",
    "branded_df = pd.read_csv(\"../DATA/branded_food.csv\")\n",
    "nutrient_df = pd.read_csv(\"../DATA/food_nutrient.csv\")\n",
    "nutrient_ref = pd.read_csv(\"../DATA/nutrient.csv\")\n",
    "food_df = pd.read_csv(\"../DATA/food.csv\")\n",
    "\n",
    "print(f\"Branded Foods Dataset: {branded_df.shape}\")\n",
    "print(f\"Food Nutrients Dataset: {nutrient_df.shape}\")\n",
    "print(f\"Nutrient Reference Dataset: {nutrient_ref.shape}\")\n",
    "print(f\"Food Reference Dataset: {food_df.shape}\")\n",
    "\n",
    "# Initial data quality assessment\n",
    "print(\"\\nData Quality Assessment:\")\n",
    "for name, df in [(\"Branded Foods\", branded_df), (\"Food Nutrients\", nutrient_df)]:\n",
    "    missing_pct = (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100\n",
    "    print(f\"{name}: {missing_pct:.1f}% missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27ad6d3",
   "metadata": {},
   "source": [
    "## Text Preprocessing for Ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1bc275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ingredients_text(text):\n",
    "    \"\"\"Clean and standardize ingredient text\"\"\"\n",
    "    if not isinstance(text, str) or pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove common prefixes\n",
    "    text = re.sub(r\"^\\s*ingredients?\\s*:\\s*\", \"\", text)\n",
    "    text = re.sub(r\"^\\s*contains\\s*:\\s*\", \"\", text)\n",
    "    \n",
    "    # Handle \"contains less than X%\" phrases\n",
    "    text = re.sub(\n",
    "        r\"\\b(?:contains\\s+less\\s+than\\s*\\d+%|\" +\n",
    "        r\"contains\\s*\\d+%\\s*or\\s*less|\" +\n",
    "        r\"\\d+%\\s*or\\s*less|\" +\n",
    "        r\"less\\s+than\\s*\\d+%)\\s*(?:of)?\\s*:?\\s*\", \n",
    "        \"\", text\n",
    "    )\n",
    "    \n",
    "    # Remove parenthetical content (processing details)\n",
    "    while True:\n",
    "        new_text = re.sub(r\"\\([^()]*\\)\", \"\", text)\n",
    "        if new_text == text:\n",
    "            break\n",
    "        text = new_text\n",
    "    \n",
    "    # Standardize separators\n",
    "    text = re.sub(r\"\\band/or\\b\", \",\", text)\n",
    "    text = text.replace(\";\", \",\")\n",
    "    \n",
    "    # Clean whitespace and commas\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\s*,\\s*\", \", \", text)\n",
    "    text = re.sub(r\"(,\\s*){2,}\", \", \", text)\n",
    "    text = text.strip(\" ,\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "def parse_ingredients_to_list(text):\n",
    "    \"\"\"Convert ingredient text to standardized list\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    \n",
    "    # Split by commas and clean each ingredient\n",
    "    ingredients = [ing.strip() for ing in text.split(',')]\n",
    "    ingredients = [ing for ing in ingredients if ing and len(ing) > 1]\n",
    "    \n",
    "    return ingredients\n",
    "\n",
    "# Apply cleaning to branded foods\n",
    "print(\"Cleaning ingredient text...\")\n",
    "branded_df['ingredients_clean'] = branded_df['ingredients'].apply(clean_ingredients_text)\n",
    "branded_df['ingredients_list'] = branded_df['ingredients_clean'].apply(parse_ingredients_to_list)\n",
    "branded_df['ingredient_count'] = branded_df['ingredients_list'].apply(len)\n",
    "\n",
    "print(f\"Processed {len(branded_df)} products\")\n",
    "print(f\"Products with ingredients: {(branded_df['ingredient_count'] > 0).sum()}\")\n",
    "print(f\"Average ingredients per product: {branded_df['ingredient_count'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394eda1c",
   "metadata": {},
   "source": [
    "## Date Processing and Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c386c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process date columns\n",
    "date_columns = ['modified_date', 'available_date', 'discontinued_date']\n",
    "\n",
    "for col in date_columns:\n",
    "    if col in branded_df.columns:\n",
    "        branded_df[col] = pd.to_datetime(branded_df[col], errors='coerce')\n",
    "        print(f\"Processed {col}: {branded_df[col].notna().sum()} valid dates\")\n",
    "\n",
    "# Extract temporal features\n",
    "if 'available_date' in branded_df.columns:\n",
    "    branded_df['year_available'] = branded_df['available_date'].dt.year\n",
    "    branded_df['month_available'] = branded_df['available_date'].dt.month\n",
    "    branded_df['is_recent'] = (branded_df['year_available'] >= 2020).astype(int)\n",
    "    \n",
    "    print(f\"Date range: {branded_df['available_date'].min()} to {branded_df['available_date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b175f5",
   "metadata": {},
   "source": [
    "## Data Type Optimization and Missing Value Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b873f12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize data types for memory efficiency\n",
    "def optimize_dtypes(df):\n",
    "    \"\"\"Optimize dataframe dtypes for memory efficiency\"\"\"\n",
    "    original_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    \n",
    "    # Convert object columns with few unique values to category\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        if col not in ['ingredients', 'ingredients_clean', 'ingredients_list']:\n",
    "            unique_ratio = df[col].nunique() / len(df)\n",
    "            if unique_ratio < 0.1:  # Less than 10% unique values\n",
    "                df[col] = df[col].astype('category')\n",
    "    \n",
    "    # Convert numeric columns\n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        if df[col].min() >= 0:\n",
    "            if df[col].max() < 255:\n",
    "                df[col] = df[col].astype('uint8')\n",
    "            elif df[col].max() < 65535:\n",
    "                df[col] = df[col].astype('uint16')\n",
    "            else:\n",
    "                df[col] = df[col].astype('uint32')\n",
    "    \n",
    "    optimized_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Memory usage: {original_memory:.1f}MB -> {optimized_memory:.1f}MB ({optimized_memory/original_memory:.1%})\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply optimization\n",
    "print(\"Optimizing branded foods dataset...\")\n",
    "branded_df = optimize_dtypes(branded_df)\n",
    "\n",
    "print(\"\\nOptimizing nutrients dataset...\")\n",
    "nutrient_df = optimize_dtypes(nutrient_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5ead5b",
   "metadata": {},
   "source": [
    "## Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598e4be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data quality\n",
    "def validate_data_quality(df, name):\n",
    "    \"\"\"Perform data quality checks\"\"\"\n",
    "    print(f\"\\nData Quality Report: {name}\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Duplicate check (exclude list columns) - sample for large datasets\n",
    "    hashable_columns = []\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            # Test if column can be hashed (exclude list columns)\n",
    "            sample_val = df[col].iloc[0] if len(df) > 0 else None\n",
    "            if not isinstance(sample_val, list):\n",
    "                hashable_columns.append(col)\n",
    "        except (TypeError, IndexError):\n",
    "            continue\n",
    "    \n",
    "    if hashable_columns:\n",
    "        # For large datasets, use sampling for duplicate check\n",
    "        if len(df) > 1_000_000:\n",
    "            sample_size = min(100_000, len(df))\n",
    "            sample_df = df[hashable_columns].sample(n=sample_size, random_state=42)\n",
    "            duplicates_in_sample = sample_df.duplicated().sum()\n",
    "            duplicate_rate = duplicates_in_sample / sample_size\n",
    "            estimated_duplicates = int(duplicate_rate * len(df))\n",
    "            print(f\"Estimated duplicate rows (from {sample_size:,} sample): ~{estimated_duplicates:,} ({duplicate_rate:.2%})\")\n",
    "        else:\n",
    "            duplicates = df[hashable_columns].duplicated().sum()\n",
    "            print(f\"Duplicate rows (excluding list columns): {duplicates}\")\n",
    "    else:\n",
    "        print(\"Cannot check duplicates - no hashable columns\")\n",
    "    \n",
    "    # Missing value summary\n",
    "    missing_summary = df.isnull().sum()\n",
    "    missing_cols = missing_summary[missing_summary > 0]\n",
    "    print(f\"Columns with missing values: {len(missing_cols)}\")\n",
    "    \n",
    "    if len(missing_cols) > 0:\n",
    "        print(\"\\nTop missing value columns:\")\n",
    "        for col, count in missing_cols.head().items():\n",
    "            pct = (count / len(df)) * 100\n",
    "            print(f\"  {col}: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Data completeness score\n",
    "    completeness = (df.notna().sum().sum() / (len(df) * len(df.columns))) * 100\n",
    "    print(f\"\\nOverall completeness: {completeness:.1f}%\")\n",
    "    \n",
    "    return completeness\n",
    "\n",
    "# Validate datasets\n",
    "branded_completeness = validate_data_quality(branded_df, \"Branded Foods\")\n",
    "nutrient_completeness = validate_data_quality(nutrient_df, \"Food Nutrients\")\n",
    "\n",
    "# Additional validation for branded foods\n",
    "print(\"\\nBranded Foods Specific Validation:\")\n",
    "print(f\"Products with valid FDC IDs: {branded_df['fdc_id'].notna().sum()}\")\n",
    "print(f\"Products with brand information: {branded_df['brand_owner'].notna().sum()}\")\n",
    "print(f\"Products with ingredients: {(branded_df['ingredient_count'] > 0).sum()}\")\n",
    "print(f\"Products with categories: {branded_df['branded_food_category'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2711ae74",
   "metadata": {},
   "source": [
    "## Export Cleaned Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d4826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export cleaned datasets\n",
    "print(\"Exporting cleaned datasets...\")\n",
    "\n",
    "# Export main cleaned dataset\n",
    "output_path = data_dir / 'branded_food_cleaned.csv'\n",
    "branded_df.to_csv(output_path, index=False)\n",
    "print(f\"Branded foods saved: {output_path}\")\n",
    "\n",
    "# Export optimized nutrients dataset\n",
    "nutrient_output_path = data_dir / 'food_nutrient_cleaned.csv'\n",
    "nutrient_df.to_csv(nutrient_output_path, index=False)\n",
    "print(f\"Food nutrients saved: {nutrient_output_path}\")\n",
    "\n",
    "# Export nutrient reference\n",
    "nutrient_ref_path = data_dir / 'nutrient_reference.csv'\n",
    "nutrient_ref.to_csv(nutrient_ref_path, index=False)\n",
    "print(f\"Nutrient reference saved: {nutrient_ref_path}\")\n",
    "\n",
    "# Create processing summary\n",
    "summary = {\n",
    "    'processing_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'original_branded_records': len(branded_df),\n",
    "    'branded_completeness_pct': branded_completeness,\n",
    "    'products_with_ingredients': (branded_df['ingredient_count'] > 0).sum(),\n",
    "    'avg_ingredients_per_product': branded_df['ingredient_count'].mean(),\n",
    "    'nutrient_records': len(nutrient_df),\n",
    "    'nutrient_completeness_pct': nutrient_completeness,\n",
    "    'date_range_start': str(branded_df['available_date'].min()) if 'available_date' in branded_df.columns else 'N/A',\n",
    "    'date_range_end': str(branded_df['available_date'].max()) if 'available_date' in branded_df.columns else 'N/A'\n",
    "}\n",
    "\n",
    "summary_path = data_dir / 'preprocessing_summary.json'\n",
    "import json\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nProcessing summary saved: {summary_path}\")\n",
    "print(\"\\nPreprocessing complete! Cleaned datasets are ready for feature engineering.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc3cc36",
   "metadata": {},
   "source": [
    "## Preprocessing Summary\n",
    "\n",
    "### Completed Tasks:\n",
    "1. **Data Loading**: Loaded all USDA FoodData Central files\n",
    "2. **Text Preprocessing**: Cleaned and standardized ingredient text\n",
    "3. **Date Processing**: Converted date columns and extracted temporal features\n",
    "4. **Data Optimization**: Optimized data types for memory efficiency\n",
    "5. **Quality Validation**: Performed comprehensive data quality checks\n",
    "6. **Export**: Saved cleaned datasets to RESULTS/processed_data/\n",
    "\n",
    "### Data Quality Improvements:\n",
    "- Standardized ingredient text formatting\n",
    "- Parsed ingredients into structured lists\n",
    "- Optimized memory usage through appropriate data types\n",
    "- Extracted temporal features from dates\n",
    "- Created comprehensive quality metrics\n",
    "\n",
    "### Output Files:\n",
    "- `branded_food_cleaned.csv`: Main cleaned dataset\n",
    "- `food_nutrient_cleaned.csv`: Cleaned nutritional data\n",
    "- `nutrient_reference.csv`: Nutrient reference table\n",
    "- `preprocessing_summary.json`: Processing metadata\n",
    "\n",
    "Next: Feature Engineering (03_Feature_Engineering.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
