{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0270cd06",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Cleaning\n",
    "## USDA FoodData Central - Branded Foods Dataset\n",
    "\n",
    "This notebook handles data cleaning, preprocessing, and preparation for analysis including:\n",
    "- Data loading and initial assessment\n",
    "- Missing value treatment\n",
    "- Data type conversions and standardization\n",
    "- Text preprocessing for ingredients\n",
    "- Data quality validation\n",
    "- Export of cleaned datasets\n",
    "\n",
    "Input: Raw CSV files from DATA directory\n",
    "Output: Cleaned datasets saved to RESULTS directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97bfac93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded and directories created\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create results directories\n",
    "results_dir = Path('../RESULTS')\n",
    "data_dir = results_dir / 'processed_data'\n",
    "figures_dir = results_dir / 'figures'\n",
    "models_dir = results_dir / 'models'\n",
    "\n",
    "for directory in [results_dir, data_dir, figures_dir, models_dir]:\n",
    "    directory.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Libraries loaded and directories created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5565333",
   "metadata": {},
   "source": [
    "## Data Loading and Initial Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2710b948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Branded Foods Dataset: (1977398, 21)\n",
      "Food Nutrients Dataset: (25652681, 11)\n",
      "Nutrient Reference Dataset: (477, 5)\n",
      "Food Reference Dataset: (1977398, 8)\n",
      "\n",
      "Data Quality Assessment:\n",
      "Branded Foods Dataset: (1977398, 21)\n",
      "Food Nutrients Dataset: (25652681, 11)\n",
      "Nutrient Reference Dataset: (477, 5)\n",
      "Food Reference Dataset: (1977398, 8)\n",
      "\n",
      "Data Quality Assessment:\n",
      "Branded Foods: 36.7% missing values\n",
      "Branded Foods: 36.7% missing values\n",
      "Food Nutrients: 54.5% missing values\n",
      "Food Nutrients: 54.5% missing values\n"
     ]
    }
   ],
   "source": [
    "# Load main datasets\n",
    "print(\"Loading datasets...\")\n",
    "branded_df = pd.read_csv(\"../DATA/branded_food.csv\")\n",
    "nutrient_df = pd.read_csv(\"../DATA/food_nutrient.csv\")\n",
    "nutrient_ref = pd.read_csv(\"../DATA/nutrient.csv\")\n",
    "food_df = pd.read_csv(\"../DATA/food.csv\")\n",
    "\n",
    "print(f\"Branded Foods Dataset: {branded_df.shape}\")\n",
    "print(f\"Food Nutrients Dataset: {nutrient_df.shape}\")\n",
    "print(f\"Nutrient Reference Dataset: {nutrient_ref.shape}\")\n",
    "print(f\"Food Reference Dataset: {food_df.shape}\")\n",
    "\n",
    "# Initial data quality assessment\n",
    "print(\"\\nData Quality Assessment:\")\n",
    "for name, df in [(\"Branded Foods\", branded_df), (\"Food Nutrients\", nutrient_df)]:\n",
    "    missing_pct = (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100\n",
    "    print(f\"{name}: {missing_pct:.1f}% missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27ad6d3",
   "metadata": {},
   "source": [
    "## Text Preprocessing for Ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a1bc275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning ingredient text...\n",
      "Processed 1977398 products\n",
      "Products with ingredients: 1971849\n",
      "Average ingredients per product: 10.6\n",
      "Processed 1977398 products\n",
      "Products with ingredients: 1971849\n",
      "Average ingredients per product: 10.6\n"
     ]
    }
   ],
   "source": [
    "def clean_ingredients_text(text):\n",
    "    \"\"\"Clean and standardize ingredient text\"\"\"\n",
    "    if not isinstance(text, str) or pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove common prefixes\n",
    "    text = re.sub(r\"^\\s*ingredients?\\s*:\\s*\", \"\", text)\n",
    "    text = re.sub(r\"^\\s*contains\\s*:\\s*\", \"\", text)\n",
    "    \n",
    "    # Handle \"contains less than X%\" phrases\n",
    "    text = re.sub(\n",
    "        r\"\\b(?:contains\\s+less\\s+than\\s*\\d+%|\" +\n",
    "        r\"contains\\s*\\d+%\\s*or\\s*less|\" +\n",
    "        r\"\\d+%\\s*or\\s*less|\" +\n",
    "        r\"less\\s+than\\s*\\d+%)\\s*(?:of)?\\s*:?\\s*\", \n",
    "        \"\", text\n",
    "    )\n",
    "    \n",
    "    # Remove parenthetical content (processing details)\n",
    "    while True:\n",
    "        new_text = re.sub(r\"\\([^()]*\\)\", \"\", text)\n",
    "        if new_text == text:\n",
    "            break\n",
    "        text = new_text\n",
    "    \n",
    "    # Standardize separators\n",
    "    text = re.sub(r\"\\band/or\\b\", \",\", text)\n",
    "    text = text.replace(\";\", \",\")\n",
    "    \n",
    "    # Clean whitespace and commas\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\s*,\\s*\", \", \", text)\n",
    "    text = re.sub(r\"(,\\s*){2,}\", \", \", text)\n",
    "    text = text.strip(\" ,\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "def parse_ingredients_to_list(text):\n",
    "    \"\"\"Convert ingredient text to standardized list\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    \n",
    "    # Split by commas and clean each ingredient\n",
    "    ingredients = [ing.strip() for ing in text.split(',')]\n",
    "    ingredients = [ing for ing in ingredients if ing and len(ing) > 1]\n",
    "    \n",
    "    return ingredients\n",
    "\n",
    "# Apply cleaning to branded foods\n",
    "print(\"Cleaning ingredient text...\")\n",
    "branded_df['ingredients_clean'] = branded_df['ingredients'].apply(clean_ingredients_text)\n",
    "branded_df['ingredients_list'] = branded_df['ingredients_clean'].apply(parse_ingredients_to_list)\n",
    "branded_df['ingredient_count'] = branded_df['ingredients_list'].apply(len)\n",
    "\n",
    "print(f\"Processed {len(branded_df)} products\")\n",
    "print(f\"Products with ingredients: {(branded_df['ingredient_count'] > 0).sum()}\")\n",
    "print(f\"Average ingredients per product: {branded_df['ingredient_count'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394eda1c",
   "metadata": {},
   "source": [
    "## Date Processing and Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c386c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed modified_date: 1977378 valid dates\n",
      "Processed available_date: 1977398 valid dates\n",
      "Processed discontinued_date: 3246 valid dates\n",
      "Date range: 2019-04-01 00:00:00 to 2025-04-24 00:00:00\n",
      "Processed discontinued_date: 3246 valid dates\n",
      "Date range: 2019-04-01 00:00:00 to 2025-04-24 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Process date columns\n",
    "date_columns = ['modified_date', 'available_date', 'discontinued_date']\n",
    "\n",
    "for col in date_columns:\n",
    "    if col in branded_df.columns:\n",
    "        branded_df[col] = pd.to_datetime(branded_df[col], errors='coerce')\n",
    "        print(f\"Processed {col}: {branded_df[col].notna().sum()} valid dates\")\n",
    "\n",
    "# Extract temporal features\n",
    "if 'available_date' in branded_df.columns:\n",
    "    branded_df['year_available'] = branded_df['available_date'].dt.year\n",
    "    branded_df['month_available'] = branded_df['available_date'].dt.month\n",
    "    branded_df['is_recent'] = (branded_df['year_available'] >= 2020).astype(int)\n",
    "    \n",
    "    print(f\"Date range: {branded_df['available_date'].min()} to {branded_df['available_date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b175f5",
   "metadata": {},
   "source": [
    "## Data Type Optimization and Missing Value Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b873f12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing branded foods dataset...\n",
      "Memory usage: 2905.4MB -> 1653.7MB (56.9%)\n",
      "\n",
      "Optimizing nutrients dataset...\n",
      "Memory usage: 2905.4MB -> 1653.7MB (56.9%)\n",
      "\n",
      "Optimizing nutrients dataset...\n",
      "Memory usage: 2152.9MB -> 1810.4MB (84.1%)\n",
      "Memory usage: 2152.9MB -> 1810.4MB (84.1%)\n"
     ]
    }
   ],
   "source": [
    "# Optimize data types for memory efficiency\n",
    "def optimize_dtypes(df):\n",
    "    \"\"\"Optimize dataframe dtypes for memory efficiency\"\"\"\n",
    "    original_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    \n",
    "    # Convert object columns with few unique values to category\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        if col not in ['ingredients', 'ingredients_clean', 'ingredients_list']:\n",
    "            unique_ratio = df[col].nunique() / len(df)\n",
    "            if unique_ratio < 0.1:  # Less than 10% unique values\n",
    "                df[col] = df[col].astype('category')\n",
    "    \n",
    "    # Convert numeric columns\n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        if df[col].min() >= 0:\n",
    "            if df[col].max() < 255:\n",
    "                df[col] = df[col].astype('uint8')\n",
    "            elif df[col].max() < 65535:\n",
    "                df[col] = df[col].astype('uint16')\n",
    "            else:\n",
    "                df[col] = df[col].astype('uint32')\n",
    "    \n",
    "    optimized_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Memory usage: {original_memory:.1f}MB -> {optimized_memory:.1f}MB ({optimized_memory/original_memory:.1%})\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply optimization\n",
    "print(\"Optimizing branded foods dataset...\")\n",
    "branded_df = optimize_dtypes(branded_df)\n",
    "\n",
    "print(\"\\nOptimizing nutrients dataset...\")\n",
    "nutrient_df = optimize_dtypes(nutrient_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5ead5b",
   "metadata": {},
   "source": [
    "## Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "598e4be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Quality Report: Branded Foods\n",
      "========================================\n",
      "Estimated duplicate rows (from 100,000 sample): ~0 (0.00%)\n",
      "Estimated duplicate rows (from 100,000 sample): ~0 (0.00%)\n",
      "Columns with missing values: 16\n",
      "\n",
      "Top missing value columns:\n",
      "  brand_owner: 17232 (0.9%)\n",
      "  brand_name: 544896 (27.6%)\n",
      "  subbrand_name: 1871607 (94.6%)\n",
      "  ingredients: 5373 (0.3%)\n",
      "  not_a_significant_source_of: 1897783 (96.0%)\n",
      "Columns with missing values: 16\n",
      "\n",
      "Top missing value columns:\n",
      "  brand_owner: 17232 (0.9%)\n",
      "  brand_name: 544896 (27.6%)\n",
      "  subbrand_name: 1871607 (94.6%)\n",
      "  ingredients: 5373 (0.3%)\n",
      "  not_a_significant_source_of: 1897783 (96.0%)\n",
      "\n",
      "Overall completeness: 71.5%\n",
      "\n",
      "Data Quality Report: Food Nutrients\n",
      "========================================\n",
      "\n",
      "Overall completeness: 71.5%\n",
      "\n",
      "Data Quality Report: Food Nutrients\n",
      "========================================\n",
      "Estimated duplicate rows (from 100,000 sample): ~0 (0.00%)\n",
      "Estimated duplicate rows (from 100,000 sample): ~0 (0.00%)\n",
      "Columns with missing values: 7\n",
      "\n",
      "Top missing value columns:\n",
      "  data_points: 25652681 (100.0%)\n",
      "  derivation_id: 830 (0.0%)\n",
      "  min: 25652681 (100.0%)\n",
      "  max: 25652681 (100.0%)\n",
      "  median: 25652681 (100.0%)\n",
      "Columns with missing values: 7\n",
      "\n",
      "Top missing value columns:\n",
      "  data_points: 25652681 (100.0%)\n",
      "  derivation_id: 830 (0.0%)\n",
      "  min: 25652681 (100.0%)\n",
      "  max: 25652681 (100.0%)\n",
      "  median: 25652681 (100.0%)\n",
      "\n",
      "Overall completeness: 45.5%\n",
      "\n",
      "Branded Foods Specific Validation:\n",
      "Products with valid FDC IDs: 1977398\n",
      "Products with brand information: 1960166\n",
      "Products with ingredients: 1971849\n",
      "Products with categories: 1966768\n",
      "\n",
      "Overall completeness: 45.5%\n",
      "\n",
      "Branded Foods Specific Validation:\n",
      "Products with valid FDC IDs: 1977398\n",
      "Products with brand information: 1960166\n",
      "Products with ingredients: 1971849\n",
      "Products with categories: 1966768\n"
     ]
    }
   ],
   "source": [
    "# Validate data quality\n",
    "def validate_data_quality(df, name):\n",
    "    \"\"\"Perform data quality checks\"\"\"\n",
    "    print(f\"\\nData Quality Report: {name}\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Duplicate check (exclude list columns) - sample for large datasets\n",
    "    hashable_columns = []\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            # Test if column can be hashed (exclude list columns)\n",
    "            sample_val = df[col].iloc[0] if len(df) > 0 else None\n",
    "            if not isinstance(sample_val, list):\n",
    "                hashable_columns.append(col)\n",
    "        except (TypeError, IndexError):\n",
    "            continue\n",
    "    \n",
    "    if hashable_columns:\n",
    "        # For large datasets, use sampling for duplicate check\n",
    "        if len(df) > 1_000_000:\n",
    "            sample_size = min(100_000, len(df))\n",
    "            sample_df = df[hashable_columns].sample(n=sample_size, random_state=42)\n",
    "            duplicates_in_sample = sample_df.duplicated().sum()\n",
    "            duplicate_rate = duplicates_in_sample / sample_size\n",
    "            estimated_duplicates = int(duplicate_rate * len(df))\n",
    "            print(f\"Estimated duplicate rows (from {sample_size:,} sample): ~{estimated_duplicates:,} ({duplicate_rate:.2%})\")\n",
    "        else:\n",
    "            duplicates = df[hashable_columns].duplicated().sum()\n",
    "            print(f\"Duplicate rows (excluding list columns): {duplicates}\")\n",
    "    else:\n",
    "        print(\"Cannot check duplicates - no hashable columns\")\n",
    "    \n",
    "    # Missing value summary\n",
    "    missing_summary = df.isnull().sum()\n",
    "    missing_cols = missing_summary[missing_summary > 0]\n",
    "    print(f\"Columns with missing values: {len(missing_cols)}\")\n",
    "    \n",
    "    if len(missing_cols) > 0:\n",
    "        print(\"\\nTop missing value columns:\")\n",
    "        for col, count in missing_cols.head().items():\n",
    "            pct = (count / len(df)) * 100\n",
    "            print(f\"  {col}: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Data completeness score\n",
    "    completeness = (df.notna().sum().sum() / (len(df) * len(df.columns))) * 100\n",
    "    print(f\"\\nOverall completeness: {completeness:.1f}%\")\n",
    "    \n",
    "    return completeness\n",
    "\n",
    "# Validate datasets\n",
    "branded_completeness = validate_data_quality(branded_df, \"Branded Foods\")\n",
    "nutrient_completeness = validate_data_quality(nutrient_df, \"Food Nutrients\")\n",
    "\n",
    "# Additional validation for branded foods\n",
    "print(\"\\nBranded Foods Specific Validation:\")\n",
    "print(f\"Products with valid FDC IDs: {branded_df['fdc_id'].notna().sum()}\")\n",
    "print(f\"Products with brand information: {branded_df['brand_owner'].notna().sum()}\")\n",
    "print(f\"Products with ingredients: {(branded_df['ingredient_count'] > 0).sum()}\")\n",
    "print(f\"Products with categories: {branded_df['branded_food_category'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2711ae74",
   "metadata": {},
   "source": [
    "## Export Cleaned Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f9d4826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting cleaned datasets...\n",
      "Branded foods saved: ..\\RESULTS\\processed_data\\branded_food_cleaned.csv\n",
      "Branded foods saved: ..\\RESULTS\\processed_data\\branded_food_cleaned.csv\n",
      "Food nutrients saved: ..\\RESULTS\\processed_data\\food_nutrient_cleaned.csv\n",
      "Nutrient reference saved: ..\\RESULTS\\processed_data\\nutrient_reference.csv\n",
      "\n",
      "Processing summary saved: ..\\RESULTS\\processed_data\\preprocessing_summary.json\n",
      "\n",
      "Preprocessing complete! Cleaned datasets are ready for feature engineering.\n",
      "Food nutrients saved: ..\\RESULTS\\processed_data\\food_nutrient_cleaned.csv\n",
      "Nutrient reference saved: ..\\RESULTS\\processed_data\\nutrient_reference.csv\n",
      "\n",
      "Processing summary saved: ..\\RESULTS\\processed_data\\preprocessing_summary.json\n",
      "\n",
      "Preprocessing complete! Cleaned datasets are ready for feature engineering.\n"
     ]
    }
   ],
   "source": [
    "# Export cleaned datasets\n",
    "print(\"Exporting cleaned datasets...\")\n",
    "\n",
    "# Export main cleaned dataset\n",
    "output_path = data_dir / 'branded_food_cleaned.csv'\n",
    "branded_df.to_csv(output_path, index=False)\n",
    "print(f\"Branded foods saved: {output_path}\")\n",
    "\n",
    "# Export optimized nutrients dataset\n",
    "nutrient_output_path = data_dir / 'food_nutrient_cleaned.csv'\n",
    "nutrient_df.to_csv(nutrient_output_path, index=False)\n",
    "print(f\"Food nutrients saved: {nutrient_output_path}\")\n",
    "\n",
    "# Export nutrient reference\n",
    "nutrient_ref_path = data_dir / 'nutrient_reference.csv'\n",
    "nutrient_ref.to_csv(nutrient_ref_path, index=False)\n",
    "print(f\"Nutrient reference saved: {nutrient_ref_path}\")\n",
    "\n",
    "# Create processing summary\n",
    "summary = {\n",
    "    'processing_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'original_branded_records': len(branded_df),\n",
    "    'branded_completeness_pct': branded_completeness,\n",
    "    'products_with_ingredients': (branded_df['ingredient_count'] > 0).sum(),\n",
    "    'avg_ingredients_per_product': branded_df['ingredient_count'].mean(),\n",
    "    'nutrient_records': len(nutrient_df),\n",
    "    'nutrient_completeness_pct': nutrient_completeness,\n",
    "    'date_range_start': str(branded_df['available_date'].min()) if 'available_date' in branded_df.columns else 'N/A',\n",
    "    'date_range_end': str(branded_df['available_date'].max()) if 'available_date' in branded_df.columns else 'N/A'\n",
    "}\n",
    "\n",
    "summary_path = data_dir / 'preprocessing_summary.json'\n",
    "import json\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nProcessing summary saved: {summary_path}\")\n",
    "print(\"\\nPreprocessing complete! Cleaned datasets are ready for feature engineering.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc3cc36",
   "metadata": {},
   "source": [
    "## Preprocessing Summary\n",
    "\n",
    "### Completed Tasks:\n",
    "1. **Data Loading**: Loaded all USDA FoodData Central files\n",
    "2. **Text Preprocessing**: Cleaned and standardized ingredient text\n",
    "3. **Date Processing**: Converted date columns and extracted temporal features\n",
    "4. **Data Optimization**: Optimized data types for memory efficiency\n",
    "5. **Quality Validation**: Performed comprehensive data quality checks\n",
    "6. **Export**: Saved cleaned datasets to RESULTS/processed_data/\n",
    "\n",
    "### Data Quality Improvements:\n",
    "- Standardized ingredient text formatting\n",
    "- Parsed ingredients into structured lists\n",
    "- Optimized memory usage through appropriate data types\n",
    "- Extracted temporal features from dates\n",
    "- Created comprehensive quality metrics\n",
    "\n",
    "### Output Files:\n",
    "- `branded_food_cleaned.csv`: Main cleaned dataset\n",
    "- `food_nutrient_cleaned.csv`: Cleaned nutritional data\n",
    "- `nutrient_reference.csv`: Nutrient reference table\n",
    "- `preprocessing_summary.json`: Processing metadata\n",
    "\n",
    "Next: Feature Engineering (03_Feature_Engineering.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
